{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo:  Predicting Glucose Levels using Mulitple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo, you will learn how to:\n",
    "* Fit multiple linear regression models using python's `sklearn` pachage.  \n",
    "* Split data into training and test.\n",
    "* Manipulate and visualize multivariable arrays.\n",
    "\n",
    "We first load the packages as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diabetes Data Example\n",
    "To illustrate the concepts, we load the well-known diabetes data set.  This dataset is included in the `sklearn.datasets` module and can be loaded as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "\n",
    "# Load the diabetes dataset\n",
    "diabetes = datasets.load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print a description of the data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(diabetes.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target values are stored in the vector `y`.  The attributes for the diabetes data are stored in a data matrix, `X`.  The size is is number of samples (442) x number of attributes (10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamp, natt = X.shape\n",
    "print(\"num samples={0:d}  num attributes={1:d}\".format(nsamp,natt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In class exercise: Print the (normalized) ages of the first 5 subjects?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "print(X[0,0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In-class exercise:   Print the attributes S1-S3 for subjects 10-15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "print(X[9:15,4:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In class exercise:  Create a scatter plot of the target variable, `y` vs. the BMI.  Does there seem to be a relation?  What about `y` vs. the age?  Which is a better predictor?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "plt.plot(X[:,2],y,'o')\n",
    "plt.xlabel('BMI')\n",
    "plt.ylabel('Target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Simple Linear Regression for Each Feature Individually\n",
    "\n",
    "As a first attempt to predict the glucouse level, we could try *one attribute at a time*.  That is, for each attribute $x_k$, we could attempt to fit a [simple linear regression](../simp_lin_reg/readme.md) model:\n",
    "$$ \\hat{y} = \\beta_{0,k} + \\beta_{1,k}x_k$$\n",
    "where $\\beta_{0,k}$ and $\\beta_{1,k}$ are the coefficients in the simple linear regression model using only the attribute $x_k$.\n",
    "\n",
    "Now, we saw in class, that the goodness of fit in a simple linear regression model is given by the coefficient of determination.  Let $R_k^2$ be the coefficient for predicting $y$ from the $k$-th predictor $x_k$:\n",
    "$$R^2_k = \\frac{|s_{x_k,y}|^2}{s_{x_k}^2 s_y^2}.$$\n",
    "The following code computes $R^2_k$ for each variable $k$ as well as the coefficients in the linear model, $\\beta_{0,k}$ and $\\beta_{1,k}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ym = np.mean(y)\n",
    "syy = np.mean((y-ym)**2)\n",
    "Rsq = np.zeros(natt)\n",
    "beta0 = np.zeros(natt)\n",
    "beta1 = np.zeros(natt)\n",
    "for k in range(natt):\n",
    "    xm = np.mean(X[:,k])\n",
    "    sxy = np.mean((X[:,k]-xm)*(y-ym))\n",
    "    sxx = np.mean((X[:,k]-xm)**2)\n",
    "    beta1[k] = sxy/sxx\n",
    "    beta0[k] = ym - beta1[k]*xm\n",
    "    Rsq[k] = (sxy)**2/sxx/syy\n",
    "    \n",
    "    print(\"{0:2d} Rsq={1:f} beta0={2:f} beta1={3:f}\".format(k,Rsq[k],beta0[k],beta1[k]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the best $R^2_k = 0.34$.  That is, only about 34% of the variance is explained by a linear model with any one variable.  We can see this somewhat poor fit in the scatter plot as well where there is a significat variation from the regression line.\n",
    "Also notice that beta0 are all the same no matter which attribute is used to predict the target!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the index of the single variable with the best R^2\n",
    "imax = np.argmax(Rsq)\n",
    "\n",
    "# Regression line over the range of x values\n",
    "xmin = np.min(X[:,imax])\n",
    "xmax = np.max(X[:,imax])\n",
    "ymin = beta0[imax] + beta1[imax]*xmin\n",
    "ymax = beta0[imax] + beta1[imax]*xmax\n",
    "plt.plot([xmin,xmax], [ymin,ymax], 'r-', linewidth=3)\n",
    "\n",
    "# Scatter plot of points\n",
    "plt.scatter(X[:,imax],y)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the calculations above could have been done without a for-loop using Python broadcasting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the means\n",
    "ym = np.mean(y) \n",
    "y1 = y-ym  # a column vecotor each minus mean\n",
    "Xm = np.mean(X,axis=0) # averaging over column, resulting a row vector of dimension natt\n",
    "X1 = X - Xm[None,:] # minus the same mean in each column\n",
    "\n",
    "\n",
    "# Compute the correlations per features\n",
    "syy = np.mean(y1**2)\n",
    "Sxx = np.mean(X1**2,axis=0) #a row vector with each element indicating the variance of one attribute\n",
    "Sxy = np.mean(X1*y1[:,None],axis=0) #a row vector with each element indicating the covarance on one attribute to the targer\n",
    "\n",
    "# Compute the coefficients and R^2 value per feature\n",
    "beta1 = Sxy/Sxx # element wise division, resulting a row vector containing  beta1 for each attribute\n",
    "beta0 = ym - beta1*Xm # element wise multiplication, resulting a row vector containing beta0 for each attribute\n",
    "Rsq = Sxy**2/Sxx/syy #a row vector containing Rsq for each attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us see whether we get the same result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(natt):\n",
    "    print(\"{0:2d} Rsq={1:f} beta0={2:f} beta1={3:f}\".format(k,Rsq[k],beta0[k],beta1[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements with a Multiple Variable Linear Model\n",
    "\n",
    "One possible way to try to improve the fit is to use multiple variables at the same time.  We can fit the multipe linear model using the `sklearn` package.  \n",
    "\n",
    "For reasons that we will explain in the next demo, we need to split the data into two parts:  one part for training the model and a second part for testing the fit.  In this example, we will use `ns_test=300` samples for training and the remaining `ns_test=442-300=142` for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_train = 300\n",
    "ns_test = nsamp - ns_train\n",
    "X_tr = X[:ns_train,:]     # Gets the first ns_train rows of X\n",
    "y_tr = y[:ns_train]       # Gets the correspoinding rows of y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fit the linear model, we first create a regression object and then fit the data with regression object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X_tr,y_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next compute the RSS (per sample) and the R^2 on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr_pred = regr.predict(X_tr)\n",
    "RSS_tr = np.mean((y_tr_pred-y_tr)**2)/(np.std(y_tr)**2)\n",
    "Rsq_tr = 1-RSS_tr\n",
    "print(\"RSS per sample = {0:f}\".format(RSS_tr))\n",
    "print(\"R^2 =            {0:f}\".format(Rsq_tr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see R^2 is higher than the best single variable model -- more than 51% of the target variance is explained by the model instead of just 34% for the best single variable model.  \n",
    "\n",
    "We also create a scatter plot of predicted vs. actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_tr,y_tr_pred)\n",
    "plt.plot([0,350],[0,350],'r')\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate on Test Data\n",
    "\n",
    "As we will explain in the next lab, one should not evaluate the performance of a model on the data on which it is trained.  It is important to see how the model works on *new* data that is independent of the training data set.  For this reason, we will evaluate the model on the *test* samples that were not used in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X[ns_train:,:]\n",
    "y_test = y[ns_train:]\n",
    "y_test_pred = regr.predict(X_test)\n",
    "RSS_test = np.mean((y_test_pred-y_test)**2)/(np.std(y_test)**2)\n",
    "Rsq_test = 1-RSS_test\n",
    "print(\"RSS per sample = {0:f}\".format(RSS_test))\n",
    "print(\"R^2 =            {0:f}\".format(Rsq_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the model predicts new samples almost as well as it did the training samples.  We can also plot the fit on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test,y_test_pred)\n",
    "plt.plot([0,350],[0,350],'r')\n",
    "plt.xlabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually Computing the Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = np.ones((ns_train,1))\n",
    "A = np.hstack((ones,X_tr))\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next use the `lstsq` method to solve $\\mathbf{y} \\approx \\mathbf{A\\beta}$.  This will find the desired least-squares fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = np.linalg.lstsq(A,y_tr,rcond=None)\n",
    "beta = out[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that `beta[0]` mathches the intercept in `regr.intercept_` from the regression fit and `beta[1:]` matches the coefficients in `regr.coef_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us verify this is the same as direct matrix calculation.  For a small problem like this, it makes no difference.  But, in general, using a matrix inverse like this is *much* slower computationally than using either the `lstsq` method or the `LinearRegression` class.  So, **do not ever** solve a least squares problem like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = np.linalg.inv(A.T.dot(A)).dot(A.T.dot(y_tr))\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## In-Class Simple Exercise\n",
    "\n",
    "You are given target values `y` and features `x1` and `x2` below.  Fit the model on the first 4 data points and test the model on the fifth data point.  You may want to use the following steps\n",
    "\n",
    "*  Construct the training training data `X_tr,y_tr`\n",
    "*  Create a regression object `regr = linear_model.LinearRegression()`\n",
    "*  Fit the model with the `regr.fit()` method\n",
    "*  Predict the value on the test value with the `regr.predict()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.array([0,1,3,5,4])\n",
    "x2 = np.array([0,0.7, 4.3, 15.1, 13.2])\n",
    "y = np.array([-2, -0.9, 1.5, 18, 13])\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
