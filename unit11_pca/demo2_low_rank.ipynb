{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo:  Low-Rank Matrix Completion with Embeddings \n",
    "\n",
    "Low-rank matrix factorizations are key in PCA approximations, recommender systems, word embeddings and many other problems in machine leanring.  In this demo, you will learn to:\n",
    "\n",
    "* Describe a low rank factorization of a matrix and the matrix completion problem\n",
    "* Describe and implement an `Embedding` layer in `keras`.\n",
    "* Build a simple neural network to perform the matrix completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low-Rank Matrix Completion\n",
    "\n",
    "A matrix `M` of shape `(n0,n1)` is said to have a *low-rank factorization* if it can be written as the product:\n",
    "\n",
    "    M=A0.dot(A1.T)\n",
    "    \n",
    "where `A0` has shape `(n0,r)`, `A1` has shape `(n1,r)` and `r << n0` and `r << n1`.  The parameter `r` is called the *rank*.  The key property of a low-rank factorization is that the matrix `M` can be represented with much smaller number of parameters.  A general `(n0,n1)` matrix `M` requires `n0*n1` parameters.  However, the parameters in the two terms in the factorization require\n",
    "\n",
    "     n0*r + n1*r = (n0+n1)*r\n",
    "     \n",
    "     \n",
    "parameters.  If `r << n0,n1` then the number of parameters in the matrix factorization is much lower.  \n",
    "\n",
    "One application of this factorization is called matrix completion.  Suppose we are given a subset of the indices, `M[i0,i1]` for a small number of locations `(i0,i1)`.  The matrix completion problem is to find the remaining matrix entries.  This is the basic problem in recommender systems.  If the matrix has a suffficiently low rank approximation, then this completion can be performed.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by loading standard packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also load packages from `tensorflow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Dense, Embedding, Lambda, Input, Flatten, Dot\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Synthetic Data\n",
    "\n",
    "To illustrate the low-rank factorization, we will create *synthetic* data from a random low-rank matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n0 = 100\n",
    "n1 = 80\n",
    "nr = 5\n",
    "A0 = np.random.normal(0,1/np.sqrt(nr), (n0,nr))\n",
    "A1 = np.random.normal(0,1/np.sqrt(nr), (n1,nr))\n",
    "M = A0.dot(A1.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data consists of random samples of this matrix.  We will sample `ns=3000` of the `n0*n1=8000` entries of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsample the data\n",
    "ns = 3000\n",
    "I0 = np.random.randint(0, n0, (ns,))\n",
    "I1 = np.random.randint(0, n1, (ns,))\n",
    "\n",
    "y = np.zeros(ns)\n",
    "for i in range(ns):\n",
    "    i0 = I0[i]\n",
    "    i1 = I1[i]\n",
    "    y[i] = A0[i0,:].dot(A1[i1,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training and test data arrays `Xtr` and `Xts` have two components:  a set of indices `I0` and a set of indices `I1`.  The output is the vector of matrix values: `y[j] = M[i0,i1]` at the location `i0=I0[j]`, `i1=I1[j]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "test_frac = 0.2\n",
    "ntr = np.round((1-test_frac)*ns).astype(int)\n",
    "nts = ns - ntr\n",
    "Xtr = [I0[:ntr], I1[:ntr]]\n",
    "ytr = y[:ntr]\n",
    "Xts = [I0[ntr:], I1[ntr:]]\n",
    "yts = y[ntr:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Low-Rank Completion\n",
    "\n",
    "We now build a simple neural network for matrix completion.  The key component is an `Embedding` layer.  An `Embedding` layers takes an integer index `i=0,1,...,n-1` and outputs a vector `W[i,:]` with some dimension `d`.  The vector `W[i,:]` is called the *embedding* of the index `i`.  The parameters in the embedding is the `n x d` matrix `W` with all the embeddings.  The dimension `d` is the *embedding dimension* and `n` is the *input dimension*.  \n",
    "\n",
    "Now, if a matrix `M` has a factorization `M=A0.dot(A1.T)` then \n",
    "\n",
    "     M[i0,i1] = A0[i0,:].dot(A1[i1,:])\n",
    "     \n",
    "So, we can learn the `A0` and `A1` via embeddings.  \n",
    "\n",
    "The details of the network are as follows.  \n",
    "\n",
    "* The inputs are `ind_in0` and `ind_in1` which are the indices `i0` and `i1`.\n",
    "* Each index goes to an `Embedding` layer which looks up the rows `A0[i0,:]` and `A1[i1,:]`.\n",
    "* The outputs of the embedding are flattened, since by default they are matrices, not vectors.\n",
    "* The  `Dot` layer takes the inner product of the two terms.\n",
    "\n",
    "There is a small amount of regularization to control the size of the terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "ind_in0 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ind_in1 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "emb0 (Embedding)                (None, 1, 5)         500         ind_in0[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "emb1 (Embedding)                (None, 1, 5)         400         ind_in1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "vec0 (Flatten)                  (None, 5)            0           emb0[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "vec1 (Flatten)                  (None, 5)            0           emb1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "product (Dot)                   (None, 1)            0           vec0[0][0]                       \n",
      "                                                                 vec1[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 900\n",
      "Trainable params: 900\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "l2_reg = 1e-8\n",
    "\n",
    "ind_in0  = Input(name='ind_in0', shape=(1,))\n",
    "emb0 = Embedding(input_dim=n0, output_dim=nr,name='emb0',\n",
    "                embeddings_regularizer=regularizers.l2(l2_reg))(ind_in0)\n",
    "vec0 = Flatten(name='vec0')(emb0)\n",
    "ind_in1  = Input(name='ind_in1', shape=(1,))\n",
    "emb1 = Embedding(input_dim=n1, output_dim=nr,name='emb1',\n",
    "                embeddings_regularizer=regularizers.l2(l2_reg))(ind_in1)\n",
    "vec1 = Flatten(name='vec1')(emb1)\n",
    "yhat = Dot(name='product',axes=1)([vec0,vec1])\n",
    "\n",
    "mod = Model([ind_in0, ind_in1], yhat)\n",
    "mod.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next compile the model and initialize the embeddings with random matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "Ainit0 = np.random.normal(0,1/np.sqrt(nr), (n0,nr))\n",
    "Ainit1 = np.random.normal(0,1/np.sqrt(nr), (n1,nr))\n",
    "opt = Adam(lr=0.01)\n",
    "mod.compile(optimizer=opt, loss='mean_absolute_error', metrics=['mean_absolute_error'])\n",
    "mod.set_weights((Ainit0,Ainit1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now fit the network.  Since we need a large number of epochs, we will turn off the verbosity and instead use a `tqdm_notebook` progress bar.  You should see the `test` loss goes to zero, which means we are getting a perfect fit.\n",
    "\n",
    "Note the large `batch_size=1000`.  This is typical for Embedding layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]/Users/peiliu/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/peiliu/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      " 68%|██████▊   | 678/1000 [01:03<00:30, 10.61it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-791057566a7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Run one epoch in the loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mytr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    335\u001b[0m                 \u001b[0meval_data_iter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initializer\u001b[0m  \u001b[0;31m# pylint: disable=pointless-statement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m                 \u001b[0meval_data_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m               validation_callbacks = cbks.configure_callbacks(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    330\u001b[0m     if (context.executing_eagerly()\n\u001b[1;32m    331\u001b[0m         or ops.get_default_graph()._building_function):  # pylint: disable=protected-access\n\u001b[0;32m--> 332\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIteratorV2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    591\u001b[0m           context.context().device_spec.device_type != \"CPU\"):\n\u001b[1;32m    592\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/cpu:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    609\u001b[0m               \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m       \u001b[0;31m# Delete the resource when this object is deleted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       self._resource_deleter = IteratorResourceDeleter(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   2914\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2915\u001b[0m         \u001b[0;34m\"MakeIterator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2916\u001b[0;31m         iterator)\n\u001b[0m\u001b[1;32m   2917\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2918\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "num_epochs = 1000\n",
    "loss = []\n",
    "val_loss = []\n",
    "\n",
    "# Loop over epochs\n",
    "for i in tqdm(range(num_epochs)):\n",
    "    \n",
    "    # Run one epoch in the loop\n",
    "    hist = mod.fit(Xtr,ytr,epochs=1,batch_size=1000, verbose=0, validation_data=(Xts,yts))\n",
    "    val_loss.append(hist.history['val_loss'])\n",
    "    loss.append(hist.history['loss'])\n",
    "    \n",
    "loss=np.array(loss).ravel()\n",
    "val_loss=np.array(val_loss).ravel()\n",
    "\n",
    "# Plot the training and test loss\n",
    "plt.plot(loss)\n",
    "plt.plot(val_loss)\n",
    "plt.grid()\n",
    "plt.legend(['train', 'test'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MAE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we display the predicted and true values on the test data.  You get a perfect match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = mod.predict(Xts)\n",
    "plt.plot(yts,yhat,'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
